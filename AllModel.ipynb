{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e58901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5f51b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: question: Who was the leader of the expedition? context: The expedition was led by Captain James Cook, a British explorer. provide answer in one line\n",
      "Generated Answer: Captain James Cook\n"
     ]
    }
   ],
   "source": [
    "# valhalla/t5-small-qa-qg-hl\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"valhalla/t5-small-qa-qg-hl\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define the input text\n",
    "question = \"Who was the leader of the expedition?\"\n",
    "context = \"The expedition was led by Captain James Cook, a British explorer. provide answer in one line\"\n",
    "\n",
    "# The model expects a specific input format for question answering\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate the answer\n",
    "# The `max_length` and `num_beams` parameters can be tuned for better results.\n",
    "outputs = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the generated tokens back into a string\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Answer: {answer}\")\n",
    "# Expected output: 'Titan'\n",
    "\n",
    "# You can use a similar approach for question generation, but the input format is different.\n",
    "# It requires highlighting the answer span with '<hl>' tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a18f8688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <answer> The Eiffel Tower <context> The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n",
      "Generated Question: The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n"
     ]
    }
   ],
   "source": [
    "# iarfmoose/t5-base-question-generator\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"iarfmoose/t5-base-question-generator\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define the answer and context\n",
    "answer = \"The Eiffel Tower\"\n",
    "context = \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\"\n",
    "\n",
    "# The model requires a specific input format\n",
    "input_text = f\"<answer> {answer} <context> {context}\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the question.\n",
    "# The `generate` method is key for text-to-text models.\n",
    "output_ids = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the generated tokens\n",
    "generated_question = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Question: {generated_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59884d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5087a718cf7242598dae1fb9217b42ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4596241bf944c6594c68de66d030b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4979280ea7c1497e8d5ececc0a6f1481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8264c25455d6485cad2e7b2e8bedb8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b9c771feee442ea3ce233e3d3cf90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80187dc22dcf48cc98156a8c9ec8531e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the leader of the expedition?\n",
      "Context: The expedition was led by Captain James Cook, a British explorer.\n",
      "Predicted Answer:  Captain James Cook\n"
     ]
    }
   ],
   "source": [
    "# deepset/roberta-base-squad2\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Define the question and context\n",
    "question = \"Who was the leader of the expedition?\"\n",
    "context = \"The expedition was led by Captain James Cook, a British explorer.\"\n",
    "\n",
    "# Tokenize the input. We combine the question and context with special tokens.\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Get the model's output\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The model's output contains logits for the start and end of the answer span\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "\n",
    "# Find the index with the highest score for both start and end\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Add 1 to include the end token\n",
    "\n",
    "# Get the tokens for the answer span\n",
    "input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "answer_tokens = input_ids[answer_start:answer_end]\n",
    "\n",
    "# Decode the tokens back to a string\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Predicted Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3855d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5660ddcf6db348e2a3ffa0094a3bd936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5233f852e9d4ed0a35c4a1f2535e488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10aa6458ba1d488a87b9aca087f05903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c9b73fc7d14a23b5e3982fac8f3fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9523118938a4966ae23f6449c9d807a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b476fac9ce654fbd964c1946bed93891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba57f90ca5348e4938ae690a50bfb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Summarize the following: The sun is the star at the center of the Solar System. It is a nearly perfect ball of hot plasma, heated to incandescence by nuclear fusion reactions in its core.\n",
      "Generated Summary: Find the sun. Find the helium. Find the helium. Find the heli\n"
     ]
    }
   ],
   "source": [
    "# google/flan-t5-small / flan-t5-base\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define the task and input text. The instruction is part of the input.\n",
    "input_text = \"Summarize the following: The sun is the star at the center of the Solar System. It is a nearly perfect ball of hot plasma, heated to incandescence by nuclear fusion reactions in its core.\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate the output\n",
    "output_ids = model.generate(input_ids)\n",
    "\n",
    "# Decode the generated text\n",
    "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40aaf1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f795d9ea66094ec6bf58ab2f42fe410a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e5ea879c34483b9f94a02a28c11d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8870e8641841400b9a1f365f490f5a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f075ff90388428e9fa2ec6ca4d51bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b02fe32c504a409a62b6633cb63ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where does the name 'squad' come from?\n",
      "Context: The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles.\n",
      "Predicted Answer: stanford question answering dataset\n"
     ]
    }
   ],
   "source": [
    "# distilbert-base-uncased-distilled-squad\n",
    "import torch\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Define the question and context\n",
    "question = \"Where does the name 'squad' come from?\"\n",
    "context = \"The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles.\"\n",
    "\n",
    "# Tokenize the inputs. DistilBERT also uses special tokens to separate question and context.\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Get the model's output\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The model returns start and end logits\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "\n",
    "# Find the indices with the highest scores\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Add 1 to include the end token\n",
    "\n",
    "# Extract the answer tokens and decode\n",
    "input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "answer_tokens = input_ids[answer_start:answer_end]\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Predicted Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPassig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
